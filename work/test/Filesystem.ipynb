{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = \"../Switchboard-Corpus/swda_data/train_set.txt\"\n",
    "#test_data = \"../Switchboard-Corpus/swda_data/test_set.txt\"\n",
    "#val_data = \"../Switchboard-Corpus/swda_data/val_set.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "swda_pass = \"../Switchboard-Corpus/swda_data/\"\n",
    "dir_list = [\"train\", \"test\", \"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob(swda_pass + \"test/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 対話ルール設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lists = []\n",
    "\n",
    "utter_user = []\n",
    "utter = []\n",
    "utter_label = []\n",
    "\n",
    "train_ = []\n",
    "test_ = []\n",
    "val_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-a5430200fd21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_label_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtrain_label_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#train_dataset.append(row.strip())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_utter_data = []\n",
    "train_user_data = []\n",
    "train_label_data = []\n",
    "test = \"\"\n",
    "\n",
    "file_pass = \"\"\n",
    "\n",
    "\n",
    "f = open(swda_pass + \"train/2015.txt\", \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    for i,v in enumerate(row.strip().split(\"|\")):\n",
    "        \n",
    "        if i==0 and train_label_data[-1] != v:\n",
    "            train_label_data.append(v)\n",
    "    #train_dataset.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "utter_user = []\n",
    "utter = []\n",
    "utter_label = []\n",
    "\n",
    "for j in train_dataset:\n",
    "    for i,v in enumerate(j.split(\"|\")):\n",
    "        \n",
    "        if i == 0:\n",
    "            utter_user.append(v)\n",
    "        elif i==1:\n",
    "            utter.append(v)\n",
    "        else:\n",
    "            utter_label.append(v)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utter_data = []\n",
    "train_user_data = []\n",
    "train_label_data = []\n",
    "text = \"\"\n",
    "\n",
    "for i,v in enumerate(utter_user):\n",
    "    \n",
    "    if utter_user[i-1] != v:\n",
    "        #print(\"実行\")\n",
    "        train_user_data.append(utter_user[i-1])\n",
    "        train_utter_data.append(text)\n",
    "        train_label_data.append(utter_label[i-1])\n",
    "        text = \"\"\n",
    "        \n",
    "    text += utter[i] + \" \"\n",
    "    #print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Have you ever gotten one of those calls that is either generated by a computer or somebody going down a list and their either offering a service or they're introducing some new product in the area and normally when they call, you're either in the shower, or you're in the middle of cooking something and you have to stop everything to run to the phone. \",\n",
       " \"Yes, yes. Is, is that one that you're talking about. \",\n",
       " \"That was the big one I'm talking about. I work weird hours, and invariably just about the time, I'm going to sleep, the phone tears off the wall. \",\n",
       " 'Uh-huh, uh-huh. ',\n",
       " 'And you are trying to crawl out of a half unconscious sleep and answer the phone, you either hear, the as soon as you say hello, you hear the click of the recording coming on, ',\n",
       " 'Uh-huh. ',\n",
       " \"or you hear somebody all ready starting, reading off a list of stuff that they've read probably a thousand times that day already. \",\n",
       " \"That's true, or the ones that are, are generated by a computer. It's just a computer voice that comes on the line. Those are the ones that I really, really hate too. \",\n",
       " \"I've even had some of them, the, they're voice activated and you've got to say hello twice before they'll do anything. \",\n",
       " \"Uh-huh, uh-huh. Yeah, sometimes I, I get them on my, uh, answering machine at home so, and I hate that when I've got a whole bunch of messages and I go through them and most of them aren't from anybody at all. \"]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_utter_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fo_o_fw_\"_by_bc', 'qy', 'sd', 'b', 'sd', 'b', 'sd', 'sd', 'sd', 'sd']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_user_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_utter_data[:-1], train_label_data[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Have you ever gotten one of those calls that is either generated by a computer or somebody going down a list and their either offering a service or they're introducing some new product in the area and normally when they call, you're either in the shower, or you're in the middle of cooking something and you have to stop everything to run to the phone. \", shape=(), dtype=string) tf.Tensor(b'qy', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "i,v = next(iter(train_data))\n",
    "print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(utter_label)\n",
    "utter_labels = []\n",
    "for i in utter_label:\n",
    "    for j,v in enumerate(labels):\n",
    "        if i == v:\n",
    "            utter_labels.append(j)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((utter, utter_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ボキャブラリーリスト\n",
    "vocabulary_set = set()\n",
    "## トークナイザー\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "## 分かち書き\n",
    "for text_tensor,_ in train_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "    \n",
    "## ボキャブラリーリスト作成\n",
    "vocab_size = len(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMAX_LENGTH = 10\\ndef filter_max_length(x,y,max_length=MAX_LENGTH):\\n    return tf.logical_and(tf.size(x) <= max_length,\\n                        tf.size(y) <= max_length)\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(token, label):\n",
    "    token = encoder.encode(token.numpy())\n",
    "    return token, label\n",
    "\n",
    "@tf.function\n",
    "def tf_encoder(utter, label):\n",
    "    encoded_text, label = tf.py_function(encode,[utter, label],[tf.int64, tf.int32])\n",
    "    encoded_text.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    return encoded_text, label\n",
    "\n",
    "\"\"\"\n",
    "MAX_LENGTH = 10\n",
    "def filter_max_length(x,y,max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data_encode = train_data.map(tf_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((None,), ()), types: (tf.int64, tf.int32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([276], shape=(1,), dtype=int64) tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(\n",
      "[153 238 267 379 300 354 392 196 342 201 367  50 177 196 177 149 196 326\n",
      " 285 196  60 369 354 336  95 169], shape=(26,), dtype=int64) tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(\n",
      "[275 305 152 288 114 367 392 288 114 306 305  93 263 405  89  77  93  44\n",
      " 367 139  71 364 354 201 367 392 368 288 114  94 367 392  24 367 392 226\n",
      " 196  14 312 196 367 392  14 312 357 259], shape=(46,), dtype=int64) tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor([106], shape=(1,), dtype=int64) tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor([198], shape=(1,), dtype=int64) tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i,v in all_train_data_encode.take(5):\n",
    "    print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data_encode = all_train_data_encode.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([276], shape=(1,), dtype=int64) tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor([106], shape=(1,), dtype=int64) tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor([198], shape=(1,), dtype=int64) tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor([153 387], shape=(2,), dtype=int64) tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor([ 26 228 125 136  77 336  95 169], shape=(8,), dtype=int64) tf.Tensor(6, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i,v in all_train_data_encode.take(5):\n",
    "    print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "max_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = all_train_data_encode.padded_batch(64, padded_shapes=([max_len], []), drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 40])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch, train_labels = next(iter(train_datasets))\n",
    "train_batch.numpy()\n",
    "train_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = tf.keras.layers.Embedding(encoder.vocab_size, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 40, 64])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = emb(train_batch)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "f = open(train_data, \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    train_dataset.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192390"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "f = open(test_data, \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    test_dataset.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4078"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = []\n",
    "f = open(val_data, \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    val_dataset.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3272"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_user = []\n",
    "utter = []\n",
    "utter_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'Okay.', 'fo_o_fw_\"_by_bc']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]\n",
    "train_dataset[0].split(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "for j in train_dataset:\n",
    "    for i,v in enumerate(j.split(\"|\")):\n",
    "        if i == 0:\n",
    "            utter_user.append(v)\n",
    "        elif i==1:\n",
    "            utter.append(v)\n",
    "        else:\n",
    "            utter_label.append(v)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英語翻訳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 蛇の目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: janome in /opt/conda/lib/python3.8/site-packages (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install janome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの取り出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_utter = []\n",
    "jp_utter = []\n",
    "path = \"./ch07/dataset/jpn.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path, \"r\", encoding='utf-8')\n",
    "text = \"\"\n",
    "max_length = 0\n",
    "count = 0\n",
    "max_length_row = 0\n",
    "\n",
    "for row in f:\n",
    "    idx = row.find(\"_\")\n",
    "    if max_length < idx:\n",
    "        max_length = idx\n",
    "        max_length_row = count\n",
    "    en_utter.append(row[:idx])\n",
    "    jp_utter.append(row[idx+1:].split(\"\\n\")[0])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配列をランダムに配置させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def rand_ints_nodup(a, b, k):\n",
    "    ns = []\n",
    "    while len(ns) < k:\n",
    "        n = random.randint(a, b)\n",
    "        if not n in ns:\n",
    "            ns.append(n)\n",
    "    return ns\n",
    "\n",
    "random_lists = rand_ints_nodup(0, len(en_utter), 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_list_jp = []\n",
    "tests_list_en = []\n",
    "\n",
    "for i in random_lists:\n",
    "    tests_list_en.append(en_utter[i])\n",
    "    tests_list_jp.append(jp_utter[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_jp = []\n",
    "train_list_en = []\n",
    "\n",
    "for i in range(len(en_utter)):\n",
    "    if i in random_lists:\n",
    "        continue\n",
    "        \n",
    "    train_list_en.append(en_utter[i])\n",
    "    train_list_jp.append(jp_utter[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50639, 5000)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list_en), len(tests_list_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークナイザ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "entokenizer = tfds.features.text.Tokenizer()\n",
    "jptokenizer = Tokenizer(wakati=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_tokens, out_tokens = [], []\n",
    "for i in en_utter:\n",
    "    inp_tokens += entokenizer.tokenize(i)\n",
    "for j in jp_utter:\n",
    "    out_tokens += jptokenizer.tokenize(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378947"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_vocab_lists = set(inp_tokens)\n",
    "out_vocab_lists = set(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10648"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp_vocab_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ボキャブサイズ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_vocab_size = len(inp_vocab_lists)\n",
    "out_vocab_size = len(out_vocab_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizaer_enutter = tfds.features.text.TokenTextEncoder(inp_vocab_lists)\n",
    "tokenizaer_jputter = tfds.features.text.TokenTextEncoder(out_vocab_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_list_en, train_list_jp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((tests_list_en, tests_list_jp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータ設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000\n",
    "MAX_LENGTH = 40\n",
    "# 埋め込みベクトルの次元\n",
    "embedding_dim = 256\n",
    "# RNN ユニットの数\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_encoder = tfds.features.text.TokenTextEncoder(inp_vocab_lists)\n",
    "jp_encoder = tfds.features.text.TokenTextEncoder(out_vocab_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en,jp):\n",
    "    en = en_encoder.encode(en.numpy())\n",
    "    jp = jp_encoder.encode(jp.numpy())\n",
    "    return en, jp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文の長さを固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encoder(en, jp):\n",
    "    return tf.py_function(encode, [en,jp], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの中身確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, None), (None, None)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_dataset.map(tf_encoder)\n",
    "#train_data = train_data.filter(filter_max_length)\n",
    "train_data = train_data.cache()\n",
    "train_data = train_data.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "train_data.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 13]), TensorShape([64, 2]))"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_data))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 1024]), TensorShape([64, 13]))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_hidden.shape, example_input_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(inp_vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 13, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     multiple                  2725888   \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  multiple                  3938304   \n",
      "=================================================================\n",
      "Total params: 6,664,192\n",
      "Trainable params: 6,664,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(out_vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((64, 1)), sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 14804])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     multiple                  3789824   \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  15174100  \n",
      "=================================================================\n",
      "Total params: 22,902,228\n",
      "Trainable params: 22,902,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## オプティマイザと損失関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チェックポイント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher Forcing - 正解値を次の入力として供給\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "        loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # Teacher Forcing を使用\n",
    "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_data.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "  # 2 エポックごとにモデル（のチェックポイント）を保存\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
