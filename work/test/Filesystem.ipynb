{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = \"../Switchboard-Corpus/swda_data/train_set.txt\"\n",
    "#test_data = \"../Switchboard-Corpus/swda_data/test_set.txt\"\n",
    "#val_data = \"../Switchboard-Corpus/swda_data/val_set.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "swda_pass = \"../Switchboard-Corpus/swda_data/\"\n",
    "dir_list = [\"train\", \"test\", \"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob(swda_pass + \"test/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 対話ルール設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lists = []\n",
    "\n",
    "utter_user = []\n",
    "utter = []\n",
    "utter_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utter_data = []\n",
    "train_user_data = []\n",
    "train_label_data = []\n",
    "test = \"\"\n",
    "\n",
    "f = open(swda_pass + \"train/2015.txt\", \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    for i,v in enumerate(row.strip().split(\"|\")):\n",
    "        \n",
    "        if i==0 and train_label_data[-1] != v:\n",
    "            train_label_data.append(v)\n",
    "    #train_dataset.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "utter_user = []\n",
    "utter = []\n",
    "utter_label = []\n",
    "\n",
    "for j in train_dataset:\n",
    "    for i,v in enumerate(j.split(\"|\")):\n",
    "        \n",
    "        if i == 0:\n",
    "            utter_user.append(v)\n",
    "        elif i==1:\n",
    "            utter.append(v)\n",
    "        else:\n",
    "            utter_label.append(v)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utter_data = []\n",
    "train_user_data = []\n",
    "train_label_data = []\n",
    "text = \"\"\n",
    "\n",
    "for i,v in enumerate(utter_user):\n",
    "    \n",
    "    if utter_user[i-1] != v:\n",
    "        #print(\"実行\")\n",
    "        train_user_data.append(utter_user[i-1])\n",
    "        train_utter_data.append(text)\n",
    "        train_label_data.append(utter_label[i-1])\n",
    "        text = \"\"\n",
    "        \n",
    "    text += utter[i] + \" \"\n",
    "    #print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Have you ever gotten one of those calls that is either generated by a computer or somebody going down a list and their either offering a service or they're introducing some new product in the area and normally when they call, you're either in the shower, or you're in the middle of cooking something and you have to stop everything to run to the phone. \",\n",
       " \"Yes, yes. Is, is that one that you're talking about. \",\n",
       " \"That was the big one I'm talking about. I work weird hours, and invariably just about the time, I'm going to sleep, the phone tears off the wall. \",\n",
       " 'Uh-huh, uh-huh. ',\n",
       " 'And you are trying to crawl out of a half unconscious sleep and answer the phone, you either hear, the as soon as you say hello, you hear the click of the recording coming on, ',\n",
       " 'Uh-huh. ',\n",
       " \"or you hear somebody all ready starting, reading off a list of stuff that they've read probably a thousand times that day already. \",\n",
       " \"That's true, or the ones that are, are generated by a computer. It's just a computer voice that comes on the line. Those are the ones that I really, really hate too. \",\n",
       " \"I've even had some of them, the, they're voice activated and you've got to say hello twice before they'll do anything. \",\n",
       " \"Uh-huh, uh-huh. Yeah, sometimes I, I get them on my, uh, answering machine at home so, and I hate that when I've got a whole bunch of messages and I go through them and most of them aren't from anybody at all. \"]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_utter_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fo_o_fw_\"_by_bc', 'qy', 'sd', 'b', 'sd', 'b', 'sd', 'sd', 'sd', 'sd']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_user_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(utter_label)\n",
    "utter_labels = []\n",
    "for i in utter_label:\n",
    "    for j,v in enumerate(labels):\n",
    "        if i == v:\n",
    "            utter_labels.append(j)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((utter, utter_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ボキャブラリーリスト\n",
    "vocabulary_set = set()\n",
    "## トークナイザー\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "## 分かち書き\n",
    "for text_tensor,_ in train_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "    \n",
    "## ボキャブラリーリスト作成\n",
    "vocab_size = len(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMAX_LENGTH = 10\\ndef filter_max_length(x,y,max_length=MAX_LENGTH):\\n    return tf.logical_and(tf.size(x) <= max_length,\\n                        tf.size(y) <= max_length)\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(token, label):\n",
    "    token = encoder.encode(token.numpy())\n",
    "    return token, label\n",
    "\n",
    "@tf.function\n",
    "def tf_encoder(utter, label):\n",
    "    encoded_text, label = tf.py_function(encode,[utter, label],[tf.int64, tf.int32])\n",
    "    encoded_text.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    return encoded_text, label\n",
    "\n",
    "\"\"\"\n",
    "MAX_LENGTH = 10\n",
    "def filter_max_length(x,y,max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data_encode = train_data.map(tf_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((None,), ()), types: (tf.int64, tf.int32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([276], shape=(1,), dtype=int64) tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(\n",
      "[153 238 267 379 300 354 392 196 342 201 367  50 177 196 177 149 196 326\n",
      " 285 196  60 369 354 336  95 169], shape=(26,), dtype=int64) tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(\n",
      "[275 305 152 288 114 367 392 288 114 306 305  93 263 405  89  77  93  44\n",
      " 367 139  71 364 354 201 367 392 368 288 114  94 367 392  24 367 392 226\n",
      " 196  14 312 196 367 392  14 312 357 259], shape=(46,), dtype=int64) tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor([106], shape=(1,), dtype=int64) tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor([198], shape=(1,), dtype=int64) tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i,v in all_train_data_encode.take(5):\n",
    "    print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data_encode = all_train_data_encode.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([276], shape=(1,), dtype=int64) tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor([106], shape=(1,), dtype=int64) tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor([198], shape=(1,), dtype=int64) tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor([153 387], shape=(2,), dtype=int64) tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor([ 26 228 125 136  77 336  95 169], shape=(8,), dtype=int64) tf.Tensor(6, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i,v in all_train_data_encode.take(5):\n",
    "    print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "max_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = all_train_data_encode.padded_batch(64, padded_shapes=([max_len], []), drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 40])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch, train_labels = next(iter(train_datasets))\n",
    "train_batch.numpy()\n",
    "train_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = tf.keras.layers.Embedding(encoder.vocab_size, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 40, 64])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = emb(train_batch)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 40, 64), dtype=float32, numpy=\n",
       "array([[[-2.8606737e-02,  2.6519645e-02, -1.4793098e-02, ...,\n",
       "          4.7928516e-02, -1.8492714e-03,  2.3488626e-03],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        ...,\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02]],\n",
       "\n",
       "       [[ 4.3721572e-03,  4.3036234e-02, -4.5510102e-02, ...,\n",
       "          2.5658082e-02, -4.5279574e-02,  3.8955696e-03],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        ...,\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02]],\n",
       "\n",
       "       [[ 2.5649894e-02, -4.6208229e-02, -3.3050850e-02, ...,\n",
       "         -4.9354624e-02, -1.9477094e-02,  3.7431005e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        ...,\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.1689484e-02,  2.4020288e-02,  4.4820789e-02, ...,\n",
       "          1.6092550e-02, -2.1982729e-02,  8.2664713e-03],\n",
       "        [-4.4346824e-03,  4.4101622e-02, -4.2279910e-02, ...,\n",
       "         -1.4101673e-02,  1.9366238e-02, -1.1457577e-03],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        ...,\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02]],\n",
       "\n",
       "       [[-4.8733629e-02, -3.1837273e-02, -3.7136257e-02, ...,\n",
       "         -3.3597253e-02, -1.1830460e-02, -3.4224726e-02],\n",
       "        [ 1.4937032e-02, -2.5399471e-02, -1.6962193e-02, ...,\n",
       "          2.6514303e-02, -1.1555862e-02,  3.8798992e-02],\n",
       "        [ 4.9829211e-02, -2.0305669e-02,  7.4936859e-03, ...,\n",
       "         -3.6190651e-02,  5.3896792e-03,  3.7698951e-02],\n",
       "        ...,\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02]],\n",
       "\n",
       "       [[ 4.8763763e-02, -3.2971609e-02,  4.7262225e-02, ...,\n",
       "          1.9558255e-02,  3.3159148e-02, -3.0985868e-02],\n",
       "        [ 2.4869218e-03, -2.2923624e-02,  3.3260833e-02, ...,\n",
       "         -1.1276454e-05,  4.0163886e-02,  1.4866140e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        ...,\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02],\n",
       "        [-3.5874616e-02,  3.6047582e-02, -2.7086401e-02, ...,\n",
       "          1.5918341e-02,  3.9573919e-02,  4.3381263e-02]]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "f = open(train_data, \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    train_dataset.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192390"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "f = open(test_data, \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    test_dataset.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4078"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = []\n",
    "f = open(val_data, \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    val_dataset.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3272"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_user = []\n",
    "utter = []\n",
    "utter_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'Okay.', 'fo_o_fw_\"_by_bc']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]\n",
    "train_dataset[0].split(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "for j in train_dataset:\n",
    "    for i,v in enumerate(j.split(\"|\")):\n",
    "        if i == 0:\n",
    "            utter_user.append(v)\n",
    "        elif i==1:\n",
    "            utter.append(v)\n",
    "        else:\n",
    "            utter_label.append(v)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ウィンドウサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = []\n",
    "lists = []\n",
    "for i in range(len(train_dataset)):\n",
    "    lists.append(train_dataset[i])\n",
    "    if i%window_size == 4:\n",
    "        train_datasets.append(lists)\n",
    "        lists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38478"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_train_dataset = tf.data.Dataset.from_tensor_slices((utter, utter_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21852"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_set = set()\n",
    "for text_tensor, _ in utter_train_dataset:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21854"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(token, label):\n",
    "    token = encoder.encode(token.numpy())\n",
    "    label = np.array([label])\n",
    "    return token, label\n",
    "\n",
    "def tf_encoder(utter, label):\n",
    "    return tf.py_function(encode, [utter, label],[tf.int64, tf.int64])\n",
    "\n",
    "MAX_LENGTH = 40\n",
    "def filter_max_length(x,y,max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, None), (None, None)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set = utter_train_dataset.map(tf_encoder)\n",
    "train_data_set = train_data_set.filter(filter_max_length)\n",
    "train_data_set = train_data_set.cache()\n",
    "train_data_set = train_data_set.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "train_data_set.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateModel(vocab_size, BATCH_SIZE):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, 300, batch_input_shape=[BATCH_SIZE, None]),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(512, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CreateModel(vocab_size, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_embedding = tf.keras.layers.Embedding(1000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tf.keras.preprocessing.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192390\n"
     ]
    }
   ],
   "source": [
    "token.fit_on_texts(utter)\n",
    "print(token.document_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data = token.texts_to_sequences(utter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## バイナリ表現\n",
    "matrix_binary = token.sequences_to_matrix(sequence_data, \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a3da6fad6a46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## カウント表現\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmatrix_binary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "## カウント表現\n",
    "matrix_count = token.sequences_to_matrix(sequence_data, \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tfidf\n",
    "matrix_tfidf = token.sequences_to_matrix(sequence_data, \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
