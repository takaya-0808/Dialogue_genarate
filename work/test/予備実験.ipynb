{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予備実験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 発話分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = \"../Switchboard-Corpus/swda_data/train_set.txt\"\n",
    "test_dataset_path = \"../Switchboard-Corpus/swda_data/test_set.txt\"\n",
    "val_dataset_path = \"../Switchboard-Corpus/swda_data/val_set.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "f = open(train_dataset_path, \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    train_dataset.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = []\n",
    "f = open(val_dataset_path, \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    val_dataset.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "f = open(test_dataset_path, \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    test_dataset.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 訓練データ\n",
    "train_utter_user = []\n",
    "train_utter = []\n",
    "train_utter_label = []\n",
    "\n",
    "## テストデータ\n",
    "test_utter_user = []\n",
    "test_utter = []\n",
    "test_utter_label = []\n",
    "\n",
    "## 検証データ\n",
    "val_utter_user = []\n",
    "val_utter = []\n",
    "val_utter_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "for j in train_dataset:\n",
    "    for i,v in enumerate(j.split(\"|\")):\n",
    "        if i == 0:\n",
    "            train_utter_user.append(v)\n",
    "        elif i==1:\n",
    "            train_utter.append(v)\n",
    "        else:\n",
    "            train_utter_label.append(v)\n",
    "            \n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "for j in test_dataset:\n",
    "    for i,v in enumerate(j.split(\"|\")):\n",
    "        if i == 0:\n",
    "            test_utter_user.append(v)\n",
    "        elif i==1:\n",
    "            test_utter.append(v)\n",
    "        else:\n",
    "            test_utter_label.append(v)\n",
    "            \n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "for j in val_dataset:\n",
    "    for i,v in enumerate(j.split(\"|\")):\n",
    "        if i == 0:\n",
    "            val_utter_user.append(v)\n",
    "        elif i==1:\n",
    "            val_utter.append(v)\n",
    "        else:\n",
    "            val_utter_label.append(v)\n",
    "            \n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ラベルデータ数値化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = \"../Switchboard-Corpus/swda_data/metadata/labels.txt\"\n",
    "labels = []\n",
    "f = open(label_path, \"r\", encoding='utf-8')\n",
    "for row in f:\n",
    "    labels.append(row.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {}\n",
    "for i,v in enumerate(labels):\n",
    "    label[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utter_labels = []\n",
    "for i in train_utter_label:\n",
    "    train_utter_labels.append(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_utter_labels = []\n",
    "for i in test_utter_label:\n",
    "    test_utter_labels.append(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_utter_labels = []\n",
    "for i in val_utter_label:\n",
    "    val_utter_labels.append(label[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットをtensorflowに扱える形にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_utter, train_utter_labels))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_utter, test_utter_labels))\n",
    "val_data = tf.data.Dataset.from_tensor_slices((val_utter, val_utter_labels))\n",
    "#test_data = tf.data.Dataset.from_tensor_slices((test_utter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Okay.', shape=(), dtype=string) tf.Tensor(17, shape=(), dtype=int32)\n",
      "tf.Tensor(b'So, What kind of experience do you, do you have, then with child care?', shape=(), dtype=string) tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(b'I guess, I think, uh, I wonder if that worked.', shape=(), dtype=string) tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(b'Does it say something?', shape=(), dtype=string) tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(b'I think it usually does.', shape=(), dtype=string) tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(b'You might try, uh,', shape=(), dtype=string) tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(b\"I don't know,\", shape=(), dtype=string) tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(b'hold it down a little longer,', shape=(), dtype=string) tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(b'and see if it, uh,', shape=(), dtype=string) tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(b'Okay', shape=(), dtype=string) tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i,v in train_data.take(10):\n",
    "    print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Okay.', shape=(), dtype=string) tf.Tensor(17, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "i,v = next(iter(train_data))\n",
    "print(i,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークナイザー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ボキャブラリーリスト\n",
    "vocabulary_set = set()\n",
    "## トークナイザー\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "## 分かち書き\n",
    "for text_tensor,_ in train_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    #print(some_tokens)\n",
    "    vocabulary_set.update(some_tokens)\n",
    "    \n",
    "## ボキャブラリーリスト作成\n",
    "vocab_size = len(vocabulary_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(token, label):\n",
    "    token = encoder.encode(token.numpy())\n",
    "    return token, label\n",
    "\n",
    "@tf.function\n",
    "def tf_encoder(utter, label):\n",
    "    encoded_text, label = tf.py_function(encode,[utter, label],[tf.int64, tf.int32])\n",
    "    encoded_text.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    return encoded_text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode(token):\n",
    "    token = encoder.encode(token.numpy())\n",
    "    return token\n",
    "\n",
    "@tf.function\n",
    "def test_tf_encoder(utter):\n",
    "    encoded_text = tf.py_function(encode,[utter],[tf.int64])\n",
    "    #encoded_text.set_shape([None])\n",
    "    \n",
    "    return encoded_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "      tf.keras.metrics.Accuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ設計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data_encode = train_data.map(tf_encoder)\n",
    "all_test_data_encode = test_data.map(tf_encoder)\n",
    "all_val_data_encode = val_data.map(tf_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_train_data = all_train_data_encode.padded_batch(64, padded_shapes=([None], []), drop_remainder=True)\n",
    "utter_test_data = all_test_data_encode.padded_batch(64, padded_shapes=([None], []), drop_remainder=True)\n",
    "utter_val_data = all_val_data_encode.padded_batch(64, padded_shapes=([None], []), drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[13492 19133     0 ...     0     0     0]\n",
      " [21587  2343  2191 ...     0     0     0]\n",
      " [11719  8328 17129 ...     0     0     0]\n",
      " ...\n",
      " [  316     0     0 ...     0     0     0]\n",
      " [20545 20106 20545 ...     0     0     0]\n",
      " [  316     0     0 ...     0     0     0]], shape=(64, 58), dtype=int64) tf.Tensor(\n",
      "[17  9 24  2 22  0  0  2  1  6 10  0  0 11  6 22  3  0  1  0  0  0  1  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0  1  2  0  0  1 16  7  7  0  0  3  2\n",
      "  0  1  2  2  0  0  1 14  7  0  0  1  5  1  2  1], shape=(64,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  164 19685  1925 ...     0     0     0]\n",
      " [ 4907  4171     0 ...     0     0     0]\n",
      " [16190  4110  5521 ...     0     0     0]\n",
      " ...\n",
      " [ 8025     0     0 ...     0     0     0]\n",
      " [21321  2792 20545 ...     0     0     0]\n",
      " [20545 20106 10585 ...     0     0     0]], shape=(64, 45), dtype=int64) tf.Tensor(\n",
      "[ 2  1  0  0  1  1  0  1  1  1  0  2  4  2 22  3  0 13 18  3  0  3 21  1\n",
      "  0  0  3 12  4  4  6  0  1  0  0  1  0  0  0  1  0  0  0  1  5  2  2  1\n",
      "  3  6 31  3 16 17 18  0 16  4  2  0  1  1 12  9], shape=(64,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 4907  4110 18538 ...     0     0     0]\n",
      " [20545  1140  8726 ...     0     0     0]\n",
      " [ 4907  4171     0 ...     0     0     0]\n",
      " ...\n",
      " [21022 21529  2343 ...     0     0     0]\n",
      " [21022  2343 14755 ...     0     0     0]\n",
      " [16781 17129 12302 ...     0     0     0]], shape=(64, 41), dtype=int64) tf.Tensor(\n",
      "[ 2  2  1  2  2  1  1  2  1  1  6  0  0  1  2  4  2 23  9  1  6 28 13  0\n",
      "  1  3  3  1  0  3  2  2  1  1  0  2  1  2  1  2  1  2  1  0  2  1  0  1\n",
      "  0  1  1  2  9 17  5 19  0  0  3  2  0  0  0  5], shape=(64,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 6971  4683 11452 ...     0     0     0]\n",
      " [  316     0     0 ...     0     0     0]\n",
      " [ 6909 19133  2343 ...     0     0     0]\n",
      " ...\n",
      " [ 2971  8328  1947 ...     0     0     0]\n",
      " [  316     0     0 ...     0     0     0]\n",
      " [ 6909  5377 19133 ...     0     0     0]], shape=(64, 40), dtype=int64) tf.Tensor(\n",
      "[ 6  7  3  2  2  0  4  0  1  2  3  3  2  4  0  0  1  3  0  5  2  4  0  2\n",
      "  2  1  2  0  3  1  2  2  2  1  2  1  2  1 16  7  2  2  2  1  9  0  0  4\n",
      "  1  0  1  0  3  3  0  0  1  0  0  0  0  0  1  3], shape=(64,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  316     0     0 ...     0     0     0]\n",
      " [20545 15676 16433 ...     0     0     0]\n",
      " [ 3898 17985     0 ...     0     0     0]\n",
      " ...\n",
      " [20545  5521 13297 ...     0     0     0]\n",
      " [  316     0     0 ...     0     0     0]\n",
      " [20545  5521 13297 ...     0     0     0]], shape=(64, 35), dtype=int64) tf.Tensor(\n",
      "[ 1  0  1  0  3  6 26  0  1  0  3  2  6  0  3  7  0 14  7  2  3  0  0  1\n",
      "  0  1  6  7  7  0  3  1  0  1  1  0  2  1  2  3  0  1  0  0  1  1  0  1\n",
      "  1  0  1  0 14  7  0  1  2  0  0  1  0  0  1  0], shape=(64,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  316     0     0 ...     0     0     0]\n",
      " [ 6909 19133     0 ...     0     0     0]\n",
      " [20545  1947 12747 ...     0     0     0]\n",
      " ...\n",
      " [21022 19133  8328 ...     0     0     0]\n",
      " [  316     0     0 ...     0     0     0]\n",
      " [  316     0     0 ...     0     0     0]], shape=(64, 26), dtype=int64) tf.Tensor(\n",
      "[ 1  3  0  0  0  3  0  3  0  1  1  0  1  0  1 24 24  9  0  0  0  3  2  4\n",
      "  4  2  0  2  0  2  2  2  1  2  1  2  2  1  0  1  1  6  7 14  3  6  7  7\n",
      "  2 14  0  0 14  2  0  1 13  7  3  0  0  2  1  1], shape=(64,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 4683  1947 12302 ...     0     0     0]\n",
      " [18681  8328     0 ...     0     0     0]\n",
      " [  316     0     0 ...     0     0     0]\n",
      " ...\n",
      " [20545 13668  4561 ...     0     0     0]\n",
      " [21022 20545 13668 ...  7747 12302 20869]\n",
      " [ 3898     0     0 ...     0     0     0]], shape=(64, 23), dtype=int64) tf.Tensor(\n",
      "[ 3 14  7  5  1  5  6  6  0  1  3  2  0  2  2 14  3  0  0  9  0  0  0  0\n",
      "  0  1  1  0  0  0  1  3  0  1  3  0  0  1  0  1  1  0 20  4  0  0  1 21\n",
      "  0 20  4  4  4  0  3  2  1  2  1 20  0  4  0  1], shape=(64,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 3898  4110  4110 ...     0     0     0]\n",
      " [ 4110 14676   335 ...     0     0     0]\n",
      " [ 6909 20545 20571 ...     0     0     0]\n",
      " ...\n",
      " [20545  7810 20545 ...     0     0     0]\n",
      " [ 6909  4683  2426 ...     0     0     0]\n",
      " [  316     0     0 ...     0     0     0]], shape=(64, 42), dtype=int64) tf.Tensor(\n",
      "[ 0  0  0  1  0  3  0 14  7 11  2 11  2  0  1  5 13  0  0  2  3  3  2  3\n",
      "  0  0  3  3 17  0  0  1  1  0  1  0  1  0  3  1  0  1  0  3  5  0  2  1\n",
      "  2  6  2  2  4  1  0  5  0  0  1  2  1  2  2  1], shape=(64,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  316     0     0 ...     0     0     0]\n",
      " [ 3898     0     0 ...     0     0     0]\n",
      " [18315  1247  4794 ...     0     0     0]\n",
      " ...\n",
      " [18516     0     0 ...     0     0     0]\n",
      " [20545 20545  2343 ...     0     0     0]\n",
      " [21022 20545  8726 ...   308  1741  2151]], shape=(64, 34), dtype=int64) tf.Tensor(\n",
      "[ 1  1  2  4  1  6  7  7  2  1  0  2 19  2  1  2  1  2  0  1  0  1  2  2\n",
      "  0  0  1  0 20  3  0  3  1  2  2  1  2  1  3  1  3  3  2  1  0  1 21  6\n",
      "  7  9  6  2  3  3 17 21  9  6  2  2  4  4  2  2], shape=(64,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[21022 20545  7810 ...     0     0     0]\n",
      " [16781 17129 17085 ...     0     0     0]\n",
      " [21022  3545 11960 ...     0     0     0]\n",
      " ...\n",
      " [21321 18876 19685 ...     0     0     0]\n",
      " [20545  8726  4683 ...     0     0     0]\n",
      " [21022 21022  5377 ...     0     0     0]], shape=(64, 28), dtype=int64) tf.Tensor(\n",
      "[ 2  4  2  1  2 22  4  0  2  2  2  2  0  1  3  2  2  4  3  2  4  4  2  2\n",
      "  4  0  3  3  0  1 21 11  3  3  2  1  2  2  1  2 20  3  4  0  0  0  0  4\n",
      "  2  1  2  0  2  1  2  2  3  1  2  4  3  2  2  2], shape=(64,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i,v in utter_test_data.take(10):\n",
    "    print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PaddedBatchDataset shapes: ((64, None), (64,)), types: (tf.int64, tf.int32)>,\n",
       " <PaddedBatchDataset shapes: ((64, None), (64,)), types: (tf.int64, tf.int32)>,\n",
       " <PaddedBatchDataset shapes: ((64, None), (64,)), types: (tf.int64, tf.int32)>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utter_train_data, utter_test_data, utter_val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル設計(RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3006/3006 [==============================] - 211s 70ms/step - loss: -171.0589 - accuracy: 0.1976 - val_loss: -309.1272 - val_accuracy: 0.1766\n",
      "Epoch 2/10\n",
      "3006/3006 [==============================] - 216s 72ms/step - loss: -879.7972 - accuracy: 0.1982 - val_loss: -1072.3658 - val_accuracy: 0.1755\n",
      "Epoch 3/10\n",
      "3006/3006 [==============================] - 250s 83ms/step - loss: -2049.8381 - accuracy: 0.1921 - val_loss: -2051.0625 - val_accuracy: 0.1755\n",
      "Epoch 4/10\n",
      "3006/3006 [==============================] - 220s 73ms/step - loss: -3771.5759 - accuracy: 0.1921 - val_loss: -3312.4087 - val_accuracy: 0.1755\n",
      "Epoch 5/10\n",
      "3006/3006 [==============================] - 214s 71ms/step - loss: -5987.7651 - accuracy: 0.1921 - val_loss: -5530.4316 - val_accuracy: 0.1755\n",
      "Epoch 6/10\n",
      "3006/3006 [==============================] - 215s 72ms/step - loss: -8015.5977 - accuracy: 0.1921 - val_loss: -6451.9243 - val_accuracy: 0.1755\n",
      "Epoch 7/10\n",
      "3006/3006 [==============================] - 207s 69ms/step - loss: -10421.9941 - accuracy: 0.1921 - val_loss: -8224.8506 - val_accuracy: 0.1755\n",
      "Epoch 8/10\n",
      "3006/3006 [==============================] - 182s 61ms/step - loss: -13031.8008 - accuracy: 0.1921 - val_loss: -10612.7305 - val_accuracy: 0.1755\n",
      "Epoch 9/10\n",
      "3006/3006 [==============================] - 216s 72ms/step - loss: -17149.3301 - accuracy: 0.1921 - val_loss: -14497.4268 - val_accuracy: 0.1755\n",
      "Epoch 10/10\n",
      "3006/3006 [==============================] - 220s 73ms/step - loss: -22985.5254 - accuracy: 0.1921 - val_loss: -18370.9785 - val_accuracy: 0.1755\n"
     ]
    }
   ],
   "source": [
    "history = rnn_model.fit(utter_train_data, epochs=10,\n",
    "                    validation_data=utter_val_data, \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.Input(shape=(None,), batch_size=BATCH_SIZE)\n",
    "embedded = tf.keras.layers.Embedding(encoder.vocab_size, 128)(X)\n",
    "lstm = tf.keras.layers.LSTM(128, dropout=0.4, recurrent_dropout=0.4)(embedded)\n",
    "fully_connected = tf.keras.layers.Dense(units=256, activation='relu')(lstm)\n",
    "Y = tf.keras.layers.Dense(41, activation='sigmoid',name='final_layer')(fully_connected)\n",
    "Rnn_Model = tf.keras.Model(inputs=X, outputs=Y)\n",
    "\n",
    "Rnn_Model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(64, None)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (64, None, 128)           2797312   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, 128)                 131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, 256)                 33024     \n",
      "_________________________________________________________________\n",
      "final_layer (Dense)          (64, 41)                  10537     \n",
      "=================================================================\n",
      "Total params: 2,972,457\n",
      "Trainable params: 2,972,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Rnn_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3006/3006 [==============================] - 555s 185ms/step - loss: 2.1355 - accuracy: 0.3776 - val_loss: 1.8957 - val_accuracy: 0.3760\n",
      "Epoch 2/5\n",
      "3006/3006 [==============================] - 603s 201ms/step - loss: 1.7352 - accuracy: 0.4221 - val_loss: 1.2599 - val_accuracy: 0.5870\n",
      "Epoch 3/5\n",
      "3006/3006 [==============================] - 522s 174ms/step - loss: 1.2926 - accuracy: 0.6083 - val_loss: 1.0871 - val_accuracy: 0.6146\n",
      "Epoch 4/5\n",
      "3006/3006 [==============================] - 563s 187ms/step - loss: 1.1837 - accuracy: 0.6378 - val_loss: 1.0391 - val_accuracy: 0.6760\n",
      "Epoch 5/5\n",
      "2088/3006 [===================>..........] - ETA: 3:10 - loss: 1.1431 - accuracy: 0.6519"
     ]
    }
   ],
   "source": [
    "history = Rnn_Model.fit(utter_train_data, epochs=5,\n",
    "                    validation_data=utter_val_data, \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル(双方向RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.Input(shape=(None,), batch_size=BATCH_SIZE)\n",
    "embedded = tf.keras.layers.Embedding(encoder.vocab_size, 128)(X)\n",
    "lstm = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(128, dropout=0.4, recurrent_dropout=0.4))(embedded)\n",
    "fully_connected = tf.keras.layers.Dense(units=256, activation='relu')(lstm)\n",
    "Y = tf.keras.layers.Dense(41, activation='sigmoid',\n",
    "                          name='final_layer')(fully_connected)\n",
    "Bidirectional_Rnn_Model = tf.keras.Model(inputs=X, outputs=Y)\n",
    "\n",
    "Bidirectional_Rnn_Model.compile(loss='sparse_categorical_crossentropy',\n",
    "                                optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(64, None)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_20 (Embedding)     (64, None, 128)           2797312   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (64, 256)                 263168    \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (64, 256)                 65792     \n",
      "_________________________________________________________________\n",
      "final_layer (Dense)          (64, 41)                  10537     \n",
      "=================================================================\n",
      "Total params: 3,136,809\n",
      "Trainable params: 3,136,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Bidirectional_Rnn_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3006/3006 [==============================] - 929s 309ms/step - loss: 1.2228 - acc: 0.6349 - val_loss: 0.9821 - val_acc: 0.7026\n",
      "Epoch 2/5\n",
      "3006/3006 [==============================] - 1135s 378ms/step - loss: 1.0892 - acc: 0.6786 - val_loss: 0.9053 - val_acc: 0.7323\n",
      "Epoch 3/5\n",
      "3006/3006 [==============================] - 885s 294ms/step - loss: 1.0096 - acc: 0.6983 - val_loss: 0.8562 - val_acc: 0.7417\n",
      "Epoch 4/5\n",
      "3006/3006 [==============================] - 923s 307ms/step - loss: 0.9574 - acc: 0.7123 - val_loss: 0.8377 - val_acc: 0.7469\n",
      "Epoch 5/5\n",
      "1849/3006 [=================>............] - ETA: 5:52 - loss: 0.9343 - acc: 0.7162"
     ]
    }
   ],
   "source": [
    "Bidirectional_history = Bidirectional_Rnn_Model.fit(utter_train_data, epochs=5,\n",
    "                    validation_data=utter_val_data, \n",
    "                    validation_steps=30,\n",
    "                   callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(Bidirectional_history, 'acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(Bidirectional_history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル(Attention + 順方向RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.Input(shape=(None,), batch_size=BATCH_SIZE)\n",
    "embedded = tf.keras.layers.Embedding(encoder.vocab_size, 64)(X)\n",
    "lstm, forward_h, forward_c = tf.keras.layers.LSTM(128,\n",
    "                                                return_sequences=True,\n",
    "                                                return_state=True, \n",
    "                                                dropout=0.4, \n",
    "                                                recurrent_dropout=0.4)(embedded)\n",
    "#state_h = tf.keras.layers.Concatenate()([forward_h, backward_h]) # 重みを結合\n",
    "context,attention_weights = Attention(64)(lstm,forward_h) # ここにAttentionレイヤを挟む\n",
    "fully_connected = tf.keras.layers.Dense(units=128, activation='relu')(context)\n",
    "Y = tf.keras.layers.Dense(41, activation='sigmoid',name='final_layer')(fully_connected)\n",
    "\n",
    "Attention_Model = tf.keras.Model(inputs=X, outputs=Y)\n",
    "\n",
    "Attention_Model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_33\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(64, None)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (64, None, 64)       1398656     input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  [(64, None, 128), (N 98816       embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         ((64, 128), (64, Non 16577       lstm_15[0][0]                    \n",
      "                                                                 lstm_15[0][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (64, 128)            16512       attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "final_layer (Dense)             (64, 41)             5289        dense_44[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,535,850\n",
      "Trainable params: 1,535,850\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Attention_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Epoch 1/5\n",
      "3006/3006 [==============================] - 457s 152ms/step - loss: 2.0822 - acc: 0.3790 - val_loss: 1.4909 - val_acc: 0.3771\n",
      "Epoch 2/5\n",
      "3006/3006 [==============================] - 19259s 6s/step - loss: 1.4189 - acc: 0.5852 - val_loss: 1.1828 - val_acc: 0.5958\n",
      "Epoch 3/5\n",
      "3006/3006 [==============================] - 424s 141ms/step - loss: 1.2924 - acc: 0.6153 - val_loss: 1.1010 - val_acc: 0.6448\n",
      "Epoch 4/5\n",
      "3006/3006 [==============================] - 459s 153ms/step - loss: 1.2157 - acc: 0.6344 - val_loss: 1.0604 - val_acc: 0.6734\n",
      "Epoch 5/5\n",
      "3006/3006 [==============================] - 423s 141ms/step - loss: 1.1707 - acc: 0.6458 - val_loss: 1.0325 - val_acc: 0.6745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b05cb0bb0>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention_Model.fit(utter_train_data, epochs=5,\n",
    "                    validation_data=utter_val_data, \n",
    "                    validation_steps=30,\n",
    "                   callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル(Attention + 双方向RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.Input(shape=(None,), batch_size=BATCH_SIZE)\n",
    "embedded = tf.keras.layers.Embedding(encoder.vocab_size, 64)(X)\n",
    "lstm, forward_h, forward_c, backward_h, backward_c = tf.keras.layers.Bidirectional(\n",
    "                                                        tf.keras.layers.LSTM(128,\n",
    "                                                                             return_sequences=True,\n",
    "                                                                             return_state=True,\n",
    "                                                                             dropout=0.4, \n",
    "                                                                             recurrent_dropout=0.4))(embedded)\n",
    "state_h = tf.keras.layers.Concatenate()([forward_h, backward_h]) # 重みを結合\n",
    "context,attention_weights = Attention(64)(lstm,state_h) # ここにAttentionレイヤを挟む\n",
    "fully_connected = tf.keras.layers.Dense(units=128, activation='relu')(context)\n",
    "Y = tf.keras.layers.Dense(41, activation='sigmoid',name='final_layer')(fully_connected)\n",
    "\n",
    "Bidirectional_Attention_Model = tf.keras.Model(inputs=X, outputs=Y)\n",
    "\n",
    "Bidirectional_Attention_Model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(64, None)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (64, None, 64)       1398656     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) [(64, None, 256), (N 197632      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           bidirectional_7[0][1]            \n",
      "                                                                 bidirectional_7[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         ((64, 256), (64, Non 32961       bidirectional_7[0][0]            \n",
      "                                                                 concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (64, 128)            32896       attention_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "final_layer (Dense)             (64, 41)             5289        dense_36[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,667,434\n",
      "Trainable params: 1,667,434\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Bidirectional_Attention_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Epoch 1/5\n",
      "3006/3006 [==============================] - 700s 233ms/step - loss: 1.3479 - acc: 0.6029 - val_loss: 1.1791 - val_acc: 0.5943\n",
      "Epoch 2/5\n",
      "3006/3006 [==============================] - 860s 286ms/step - loss: 1.2637 - acc: 0.6106 - val_loss: 1.0910 - val_acc: 0.6031\n",
      "Epoch 3/5\n",
      "3006/3006 [==============================] - 734s 244ms/step - loss: 1.2068 - acc: 0.6152 - val_loss: 1.0744 - val_acc: 0.5964\n",
      "Epoch 4/5\n",
      "3006/3006 [==============================] - 824s 274ms/step - loss: 1.2044 - acc: 0.6162 - val_loss: 1.0622 - val_acc: 0.6219\n",
      "Epoch 5/5\n",
      "3006/3006 [==============================] - 28529s 9s/step - loss: 1.1560 - acc: 0.6432 - val_loss: 1.0225 - val_acc: 0.6849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b105339d0>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bidirectional_Attention_Model.fit(utter_train_data, epochs=5,\n",
    "                    validation_data=utter_val_data, \n",
    "                    validation_steps=30,\n",
    "                   callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(Bidirectional_Attention_Model, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(Bidirectional_Attention_Model, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル ver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attetntion_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(64,)),\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    SeqSelfAttention(attention_activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(units=41)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attetntion_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 64, 64)            1398656   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_1 (SeqSel (None, None, 64)          4161      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, None, 41)          2665      \n",
      "=================================================================\n",
      "Total params: 1,405,482\n",
      "Trainable params: 1,405,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "self_attetntion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:748 train_step\n        loss = self.compiled_loss(\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (64, 1) and (64, 64, 41) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-c1ec1c2f25b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m self_attetntion_model.fit(utter_train_data, epochs=5,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutter_val_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    callbacks=[tensorboard_callback])\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 696\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    697\u001b[0m             *args, **kwds))\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3065\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:748 train_step\n        loss = self.compiled_loss(\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (64, 1) and (64, 64, 41) are incompatible\n"
     ]
    }
   ],
   "source": [
    "self_attetntion_model.fit(utter_train_data, epochs=5,\n",
    "                    validation_data=utter_val_data, \n",
    "                    validation_steps=30,\n",
    "                   callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意単語の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_with_w_model = tf.keras.Model(inputs=Attention_Model.input, \n",
    "                               outputs=[Attention_Model.output, \n",
    "                                Attention_Model.get_layer('attention_7').output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_with_w_model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
